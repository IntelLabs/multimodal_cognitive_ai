# DepthFusion Demo

DepthFusion demo application showcases the power of the LDM3D diffusion model in generating 360 views from text prompts. UsingÂ  text prompts provided by the user, the LDM3D diffusion model generates a 2D RGB image and its corresponding depth map, providing an in-depth RGBD representation of the text prompt.

LDM3D model is a specialized version of the Stable Diffusion v1.4 model that has been modified to fit both image and depth map data. This model was then fine-tuned on a subset of the LAION400M dataset, a large-scale image-caption dataset. The depth maps used to finetune our model were generated by the [MiDaS 3.1](https://github.com/isl-org/MiDaS), dpt_beit_large, depth estimation algorithm, which provides highly accurate depth information for each pixel in an image.

We take the generated 2D RGB image and depth map and use them to compute a 360 projection using [TouchDesigner](https://derivative.ca). TouchDesigner is a versatile platform that allows for the creation of immersive and interactive multimedia experiences. Our application harnesses the power of TouchDesigner to bring the generated 360 views to life, providing users with a unique and engaging way to experience their text prompts.

Whether it's a description of a tranquil forest, a noisy cityscape, or a futuristic sci-fi world, our DepthFusion can bring these concepts to life in vivid detail.

[![](https://github.com/IntelLabs/multimodal_cognitive_ai/blob/main/Demos/DepthFusion/demo_video_YT.jpg)](https://www.youtube.com/watch?v=CgxDmH5dLao)

More views are available [here](https://www.youtube.com/watch?v=XchXw6W4Xr0).

Demo application preview:

 ![](https://github.com/IntelLabs/multimodal_cognitive_ai/blob/main/Demos/DepthFusion/LDM3D_2.gif)
