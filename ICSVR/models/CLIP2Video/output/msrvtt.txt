2023-04-23 23:46:56,234:INFO: device: cuda:0 n_gpu: 8
2023-04-23 23:46:56,319:INFO: Model loaded fail /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt.bin/pytorch_model.bin.2
2023-04-24 00:03:54,111:INFO: device: cuda:0 n_gpu: 8
2023-04-24 00:03:54,196:INFO: Model loaded fail /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt.bin/pytorch_model.bin.2
2023-04-24 00:03:54,565:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 00:03:54,566:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 00:03:54,566:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 00:03:54,566:WARNING: 	 embed_dim: 512
2023-04-24 00:03:54,566:WARNING: 	 image_resolution: 224
2023-04-24 00:03:54,566:WARNING: 	 vision_layers: 12
2023-04-24 00:03:54,566:WARNING: 	 vision_width: 768
2023-04-24 00:03:54,566:WARNING: 	 vision_patch_size: 32
2023-04-24 00:03:54,566:WARNING: 	 context_length: 77
2023-04-24 00:03:54,566:WARNING: 	 vocab_size: 49408
2023-04-24 00:03:54,566:WARNING: 	 transformer_width: 512
2023-04-24 00:03:54,566:WARNING: 	 transformer_heads: 8
2023-04-24 00:03:54,566:WARNING: 	 transformer_layers: 12
2023-04-24 00:03:54,566:WARNING: 	 cut_top_layer: 0
2023-04-24 00:03:56,433:WARNING: 	 sim_type: seqTransf
2023-04-24 00:04:03,496:INFO: --------------------
2023-04-24 00:04:03,498:INFO: Weights of CLIP2Video not initialized from pretrained model: 
   weight_center
   emb_center
   trans_layernorm.weight
   trans_layernorm.bias
2023-04-24 00:04:05,889:INFO: ***** Running test *****
2023-04-24 00:04:05,889:INFO:   Num examples = 1000
2023-04-24 00:04:05,889:INFO:   Batch size = 64
2023-04-24 00:04:05,889:INFO:   Num steps = 16
2023-04-24 00:04:48,032:INFO: sim matrix size: 1000, 1000
2023-04-24 00:04:48,132:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-24 00:04:48,132:INFO: Text-to-Video:
2023-04-24 00:04:48,132:INFO: 	>>>  R@1: 26.4 - R@5: 46.9 - R@10: 56.0 - Median R: 7.0 - Mean R: 66.2
2023-04-24 00:04:48,132:INFO: Video-to-Text:
2023-04-24 00:04:48,132:INFO: 	>>>  V2T$R@1: 24.4 - V2T$R@5: 46.2 - V2T$R@10: 58.2 - V2T$Median R: 7.0 - V2T$Mean R: 44.0
2023-04-24 00:18:09,340:INFO: device: cuda:0 n_gpu: 8
2023-04-24 00:18:09,427:INFO: Model loaded fail /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2/pytorch_model.bin.2
2023-04-24 00:18:09,806:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 00:18:09,806:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 00:18:09,806:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 00:18:09,806:WARNING: 	 embed_dim: 512
2023-04-24 00:18:09,806:WARNING: 	 image_resolution: 224
2023-04-24 00:18:09,806:WARNING: 	 vision_layers: 12
2023-04-24 00:18:09,806:WARNING: 	 vision_width: 768
2023-04-24 00:18:09,806:WARNING: 	 vision_patch_size: 32
2023-04-24 00:18:09,806:WARNING: 	 context_length: 77
2023-04-24 00:18:09,806:WARNING: 	 vocab_size: 49408
2023-04-24 00:18:09,806:WARNING: 	 transformer_width: 512
2023-04-24 00:18:09,806:WARNING: 	 transformer_heads: 8
2023-04-24 00:18:09,807:WARNING: 	 transformer_layers: 12
2023-04-24 00:18:09,807:WARNING: 	 cut_top_layer: 0
2023-04-24 00:18:11,663:WARNING: 	 sim_type: seqTransf
2023-04-24 00:18:18,648:INFO: --------------------
2023-04-24 00:18:18,649:INFO: Weights of CLIP2Video not initialized from pretrained model: 
   weight_center
   emb_center
   trans_layernorm.weight
   trans_layernorm.bias
2023-04-24 00:18:46,522:INFO: device: cuda:0 n_gpu: 8
2023-04-24 00:18:46,786:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2
2023-04-24 00:18:47,155:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 00:18:47,155:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 00:18:47,155:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 00:18:47,156:WARNING: 	 embed_dim: 512
2023-04-24 00:18:47,156:WARNING: 	 image_resolution: 224
2023-04-24 00:18:47,156:WARNING: 	 vision_layers: 12
2023-04-24 00:18:47,156:WARNING: 	 vision_width: 768
2023-04-24 00:18:47,156:WARNING: 	 vision_patch_size: 32
2023-04-24 00:18:47,156:WARNING: 	 context_length: 77
2023-04-24 00:18:47,156:WARNING: 	 vocab_size: 49408
2023-04-24 00:18:47,156:WARNING: 	 transformer_width: 512
2023-04-24 00:18:47,156:WARNING: 	 transformer_heads: 8
2023-04-24 00:18:47,156:WARNING: 	 transformer_layers: 12
2023-04-24 00:18:47,156:WARNING: 	 cut_top_layer: 0
2023-04-24 00:18:49,412:WARNING: 	 sim_type: seqTransf
2023-04-24 00:18:56,415:INFO: --------------------
2023-04-24 00:18:58,776:INFO: ***** Running test *****
2023-04-24 00:18:58,777:INFO:   Num examples = 1000
2023-04-24 00:18:58,777:INFO:   Batch size = 64
2023-04-24 00:18:58,777:INFO:   Num steps = 16
2023-04-24 00:19:38,556:INFO: sim matrix size: 1000, 1000
2023-04-24 00:19:38,655:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-24 00:19:38,655:INFO: Text-to-Video:
2023-04-24 00:19:38,655:INFO: 	>>>  R@1: 46.0 - R@5: 71.6 - R@10: 81.6 - Median R: 2.0 - Mean R: 14.5
2023-04-24 00:19:38,655:INFO: Video-to-Text:
2023-04-24 00:19:38,655:INFO: 	>>>  V2T$R@1: 43.8 - V2T$R@5: 72.7 - V2T$R@10: 82.5 - V2T$Median R: 2.0 - V2T$Mean R: 10.2
2023-04-24 00:21:27,791:INFO: device: cuda:0 n_gpu: 8
2023-04-24 00:21:28,056:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2
2023-04-24 00:21:28,426:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 00:21:28,426:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 00:21:28,426:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 00:21:28,427:WARNING: 	 embed_dim: 512
2023-04-24 00:21:28,427:WARNING: 	 image_resolution: 224
2023-04-24 00:21:28,427:WARNING: 	 vision_layers: 12
2023-04-24 00:21:28,427:WARNING: 	 vision_width: 768
2023-04-24 00:21:28,427:WARNING: 	 vision_patch_size: 32
2023-04-24 00:21:28,427:WARNING: 	 context_length: 77
2023-04-24 00:21:28,427:WARNING: 	 vocab_size: 49408
2023-04-24 00:21:28,427:WARNING: 	 transformer_width: 512
2023-04-24 00:21:28,427:WARNING: 	 transformer_heads: 8
2023-04-24 00:21:28,427:WARNING: 	 transformer_layers: 12
2023-04-24 00:21:28,427:WARNING: 	 cut_top_layer: 0
2023-04-24 00:21:30,358:WARNING: 	 sim_type: seqTransf
2023-04-24 00:21:37,447:INFO: --------------------
2023-04-24 00:21:40,024:INFO: ***** Running test *****
2023-04-24 00:21:40,024:INFO:   Num examples = 1000
2023-04-24 00:21:40,024:INFO:   Batch size = 64
2023-04-24 00:21:40,024:INFO:   Num steps = 16
2023-04-24 00:22:20,207:INFO: sim matrix size: 1000, 1000
2023-04-24 00:22:20,313:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-24 00:22:20,314:INFO: Text-to-Video:
2023-04-24 00:22:20,314:INFO: 	>>>  R@1: 46.0 - R@5: 71.6 - R@10: 81.6 - Median R: 2.0 - Mean R: 14.5
2023-04-24 00:22:20,314:INFO: Video-to-Text:
2023-04-24 00:22:20,314:INFO: 	>>>  V2T$R@1: 43.8 - V2T$R@5: 72.7 - V2T$R@10: 82.5 - V2T$Median R: 2.0 - V2T$Mean R: 10.2
2023-04-24 00:23:24,700:INFO: device: cuda:0 n_gpu: 8
2023-04-24 00:23:24,952:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2
2023-04-24 00:23:25,320:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 00:23:25,320:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 00:23:25,320:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 00:23:25,321:WARNING: 	 embed_dim: 512
2023-04-24 00:23:25,321:WARNING: 	 image_resolution: 224
2023-04-24 00:23:25,321:WARNING: 	 vision_layers: 12
2023-04-24 00:23:25,321:WARNING: 	 vision_width: 768
2023-04-24 00:23:25,321:WARNING: 	 vision_patch_size: 32
2023-04-24 00:23:25,321:WARNING: 	 context_length: 77
2023-04-24 00:23:25,321:WARNING: 	 vocab_size: 49408
2023-04-24 00:23:25,321:WARNING: 	 transformer_width: 512
2023-04-24 00:23:25,321:WARNING: 	 transformer_heads: 8
2023-04-24 00:23:25,321:WARNING: 	 transformer_layers: 12
2023-04-24 00:23:25,321:WARNING: 	 cut_top_layer: 0
2023-04-24 00:23:27,217:WARNING: 	 sim_type: seqTransf
2023-04-24 00:23:34,254:INFO: --------------------
2023-04-24 00:23:36,615:INFO: ***** Running test *****
2023-04-24 00:23:36,615:INFO:   Num examples = 1000
2023-04-24 00:23:36,615:INFO:   Batch size = 64
2023-04-24 00:23:36,615:INFO:   Num steps = 16
2023-04-24 00:24:17,531:INFO: sim matrix size: 1000, 1000
2023-04-24 00:24:17,633:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-24 00:24:17,633:INFO: Text-to-Video:
2023-04-24 00:24:17,633:INFO: 	>>>  R@1: 46.0 - R@5: 71.6 - R@10: 81.6 - Median R: 2.0 - Mean R: 14.5
2023-04-24 00:24:17,633:INFO: Video-to-Text:
2023-04-24 00:24:17,633:INFO: 	>>>  V2T$R@1: 43.8 - V2T$R@5: 72.7 - V2T$R@10: 82.5 - V2T$Median R: 2.0 - V2T$Mean R: 10.2
2023-04-24 00:27:13,448:INFO: device: cuda:0 n_gpu: 8
2023-04-24 00:27:13,700:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2
2023-04-24 00:27:14,067:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 00:27:14,067:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 00:27:14,067:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 00:27:14,068:WARNING: 	 embed_dim: 512
2023-04-24 00:27:14,068:WARNING: 	 image_resolution: 224
2023-04-24 00:27:14,068:WARNING: 	 vision_layers: 12
2023-04-24 00:27:14,068:WARNING: 	 vision_width: 768
2023-04-24 00:27:14,068:WARNING: 	 vision_patch_size: 32
2023-04-24 00:27:14,068:WARNING: 	 context_length: 77
2023-04-24 00:27:14,068:WARNING: 	 vocab_size: 49408
2023-04-24 00:27:14,068:WARNING: 	 transformer_width: 512
2023-04-24 00:27:14,068:WARNING: 	 transformer_heads: 8
2023-04-24 00:27:14,068:WARNING: 	 transformer_layers: 12
2023-04-24 00:27:14,068:WARNING: 	 cut_top_layer: 0
2023-04-24 00:27:16,014:WARNING: 	 sim_type: seqTransf
2023-04-24 00:27:23,098:INFO: --------------------
2023-04-24 00:27:25,449:INFO: ***** Running test *****
2023-04-24 00:27:25,449:INFO:   Num examples = 1000
2023-04-24 00:27:25,449:INFO:   Batch size = 64
2023-04-24 00:27:25,449:INFO:   Num steps = 16
2023-04-24 00:28:04,703:INFO: sim matrix size: 1000, 1000
2023-04-24 00:28:04,803:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-24 00:28:04,804:INFO: Text-to-Video:
2023-04-24 00:28:04,804:INFO: 	>>>  R@1: 46.0 - R@5: 71.6 - R@10: 81.6 - Median R: 2.0 - Mean R: 14.5
2023-04-24 00:28:04,804:INFO: Video-to-Text:
2023-04-24 00:28:04,804:INFO: 	>>>  V2T$R@1: 43.8 - V2T$R@5: 72.7 - V2T$R@10: 82.5 - V2T$Median R: 2.0 - V2T$Mean R: 10.2
2023-04-24 00:38:23,950:INFO: device: cuda:0 n_gpu: 8
2023-04-24 00:38:24,190:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msvd/pytorch_model.bin.2
2023-04-24 00:38:24,558:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 00:38:24,559:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 00:38:24,559:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 00:38:24,559:WARNING: 	 embed_dim: 512
2023-04-24 00:38:24,559:WARNING: 	 image_resolution: 224
2023-04-24 00:38:24,559:WARNING: 	 vision_layers: 12
2023-04-24 00:38:24,559:WARNING: 	 vision_width: 768
2023-04-24 00:38:24,559:WARNING: 	 vision_patch_size: 32
2023-04-24 00:38:24,559:WARNING: 	 context_length: 77
2023-04-24 00:38:24,559:WARNING: 	 vocab_size: 49408
2023-04-24 00:38:24,559:WARNING: 	 transformer_width: 512
2023-04-24 00:38:24,559:WARNING: 	 transformer_heads: 8
2023-04-24 00:38:24,559:WARNING: 	 transformer_layers: 12
2023-04-24 00:38:24,559:WARNING: 	 cut_top_layer: 0
2023-04-24 00:38:26,560:WARNING: 	 sim_type: seqTransf
2023-04-24 00:38:33,627:INFO: --------------------
2023-04-24 00:40:03,340:INFO: device: cuda:0 n_gpu: 8
2023-04-24 00:40:03,592:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msvd/pytorch_model.bin.2
2023-04-24 00:40:03,961:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 00:40:03,961:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 00:40:03,961:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 00:40:03,962:WARNING: 	 embed_dim: 512
2023-04-24 00:40:03,962:WARNING: 	 image_resolution: 224
2023-04-24 00:40:03,962:WARNING: 	 vision_layers: 12
2023-04-24 00:40:03,962:WARNING: 	 vision_width: 768
2023-04-24 00:40:03,962:WARNING: 	 vision_patch_size: 32
2023-04-24 00:40:03,962:WARNING: 	 context_length: 77
2023-04-24 00:40:03,962:WARNING: 	 vocab_size: 49408
2023-04-24 00:40:03,962:WARNING: 	 transformer_width: 512
2023-04-24 00:40:03,962:WARNING: 	 transformer_heads: 8
2023-04-24 00:40:03,962:WARNING: 	 transformer_layers: 12
2023-04-24 00:40:03,962:WARNING: 	 cut_top_layer: 0
2023-04-24 00:40:05,893:WARNING: 	 sim_type: seqTransf
2023-04-24 00:40:13,156:INFO: --------------------
2023-04-24 00:44:49,834:INFO: device: cuda:0 n_gpu: 8
2023-04-24 00:44:50,074:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msvd/pytorch_model.bin.2
2023-04-24 00:44:50,443:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 00:44:50,443:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 00:44:50,443:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 00:44:50,443:WARNING: 	 embed_dim: 512
2023-04-24 00:44:50,443:WARNING: 	 image_resolution: 224
2023-04-24 00:44:50,443:WARNING: 	 vision_layers: 12
2023-04-24 00:44:50,444:WARNING: 	 vision_width: 768
2023-04-24 00:44:50,444:WARNING: 	 vision_patch_size: 32
2023-04-24 00:44:50,444:WARNING: 	 context_length: 77
2023-04-24 00:44:50,444:WARNING: 	 vocab_size: 49408
2023-04-24 00:44:50,444:WARNING: 	 transformer_width: 512
2023-04-24 00:44:50,444:WARNING: 	 transformer_heads: 8
2023-04-24 00:44:50,444:WARNING: 	 transformer_layers: 12
2023-04-24 00:44:50,444:WARNING: 	 cut_top_layer: 0
2023-04-24 00:44:52,433:WARNING: 	 sim_type: seqTransf
2023-04-24 00:44:59,571:INFO: --------------------
2023-04-24 00:45:02,166:INFO: ***** Running test *****
2023-04-24 00:45:02,166:INFO:   Num examples = 27763
2023-04-24 00:45:02,166:INFO:   Batch size = 64
2023-04-24 00:45:02,166:INFO:   Num steps = 434
2023-04-24 00:45:02,172:WARNING: Eval under the multi-sentence per video clip setting.
2023-04-24 00:45:02,172:WARNING: sentence num: 27763, video num: 670
2023-04-24 00:49:38,511:INFO: device: cuda:0 n_gpu: 8
2023-04-24 00:49:38,759:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msvd/pytorch_model.bin.2
2023-04-24 00:49:39,143:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 00:49:39,144:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 00:49:39,144:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 00:49:39,144:WARNING: 	 embed_dim: 512
2023-04-24 00:49:39,144:WARNING: 	 image_resolution: 224
2023-04-24 00:49:39,144:WARNING: 	 vision_layers: 12
2023-04-24 00:49:39,144:WARNING: 	 vision_width: 768
2023-04-24 00:49:39,144:WARNING: 	 vision_patch_size: 32
2023-04-24 00:49:39,144:WARNING: 	 context_length: 77
2023-04-24 00:49:39,144:WARNING: 	 vocab_size: 49408
2023-04-24 00:49:39,144:WARNING: 	 transformer_width: 512
2023-04-24 00:49:39,144:WARNING: 	 transformer_heads: 8
2023-04-24 00:49:39,144:WARNING: 	 transformer_layers: 12
2023-04-24 00:49:39,144:WARNING: 	 cut_top_layer: 0
2023-04-24 00:49:41,895:WARNING: 	 sim_type: seqTransf
2023-04-24 00:49:49,998:INFO: --------------------
2023-04-24 00:49:52,611:INFO: ***** Running test *****
2023-04-24 00:49:52,611:INFO:   Num examples = 27763
2023-04-24 00:49:52,611:INFO:   Batch size = 64
2023-04-24 00:49:52,611:INFO:   Num steps = 434
2023-04-24 00:49:52,617:WARNING: Eval under the multi-sentence per video clip setting.
2023-04-24 00:49:52,617:WARNING: sentence num: 27763, video num: 670
2023-04-24 01:23:37,419:INFO: device: cuda:0 n_gpu: 8
2023-04-24 01:23:37,713:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msvd/pytorch_model.bin.2
2023-04-24 01:23:38,107:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 01:23:38,107:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 01:23:38,107:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 01:23:38,108:WARNING: 	 embed_dim: 512
2023-04-24 01:23:38,108:WARNING: 	 image_resolution: 224
2023-04-24 01:23:38,108:WARNING: 	 vision_layers: 12
2023-04-24 01:23:38,108:WARNING: 	 vision_width: 768
2023-04-24 01:23:38,108:WARNING: 	 vision_patch_size: 32
2023-04-24 01:23:38,108:WARNING: 	 context_length: 77
2023-04-24 01:23:38,108:WARNING: 	 vocab_size: 49408
2023-04-24 01:23:38,108:WARNING: 	 transformer_width: 512
2023-04-24 01:23:38,108:WARNING: 	 transformer_heads: 8
2023-04-24 01:23:38,108:WARNING: 	 transformer_layers: 12
2023-04-24 01:23:38,108:WARNING: 	 cut_top_layer: 0
2023-04-24 01:23:40,075:WARNING: 	 sim_type: seqTransf
2023-04-24 01:23:47,089:INFO: --------------------
2023-04-24 01:23:54,019:INFO: ***** Running test *****
2023-04-24 01:23:54,019:INFO:   Num examples = 27763
2023-04-24 01:23:54,019:INFO:   Batch size = 64
2023-04-24 01:23:54,019:INFO:   Num steps = 434
2023-04-24 01:23:54,023:WARNING: Eval under the multi-sentence per video clip setting.
2023-04-24 01:23:54,023:WARNING: sentence num: 27763, video num: 670
2023-04-24 01:24:09,591:INFO: device: cuda:0 n_gpu: 8
2023-04-24 01:24:09,851:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msvd/pytorch_model.bin.2
2023-04-24 01:24:10,216:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 01:24:10,216:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 01:24:10,216:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 01:24:10,217:WARNING: 	 embed_dim: 512
2023-04-24 01:24:10,217:WARNING: 	 image_resolution: 224
2023-04-24 01:24:10,217:WARNING: 	 vision_layers: 12
2023-04-24 01:24:10,217:WARNING: 	 vision_width: 768
2023-04-24 01:24:10,217:WARNING: 	 vision_patch_size: 32
2023-04-24 01:24:10,217:WARNING: 	 context_length: 77
2023-04-24 01:24:10,217:WARNING: 	 vocab_size: 49408
2023-04-24 01:24:10,217:WARNING: 	 transformer_width: 512
2023-04-24 01:24:10,217:WARNING: 	 transformer_heads: 8
2023-04-24 01:24:10,217:WARNING: 	 transformer_layers: 12
2023-04-24 01:24:10,217:WARNING: 	 cut_top_layer: 0
2023-04-24 01:24:12,930:WARNING: 	 sim_type: seqTransf
2023-04-24 01:24:21,011:INFO: --------------------
2023-04-24 01:24:23,903:INFO: ***** Running test *****
2023-04-24 01:24:23,903:INFO:   Num examples = 27763
2023-04-24 01:24:23,903:INFO:   Batch size = 64
2023-04-24 01:24:23,903:INFO:   Num steps = 434
2023-04-24 01:24:23,907:WARNING: Eval under the multi-sentence per video clip setting.
2023-04-24 01:24:23,907:WARNING: sentence num: 27763, video num: 670
2023-04-24 02:14:41,279:INFO: before reshape, sim matrix size: 27763 x 670
2023-04-24 02:14:41,413:INFO: after reshape, sim matrix size: 670 x 81 x 670
2023-04-24 02:14:43,433:INFO: Text-to-Video:
2023-04-24 02:14:43,433:INFO: 	>>>  R@1: 47.1 - R@5: 77.1 - R@10: 85.6 - Median R: 2.0 - Mean R: 9.6
2023-04-24 02:14:43,433:INFO: Video-to-Text:
2023-04-24 02:14:43,434:INFO: 	>>>  V2T$R@1: 59.5 - V2T$R@5: 84.6 - V2T$R@10: 90.8 - V2T$Median R: 1.0 - V2T$Mean R: 4.7
2023-04-24 02:15:06,255:INFO: before reshape, sim matrix size: 27763 x 670
2023-04-24 02:15:06,393:INFO: after reshape, sim matrix size: 670 x 81 x 670
2023-04-24 02:15:08,369:INFO: Text-to-Video:
2023-04-24 02:15:08,369:INFO: 	>>>  R@1: 43.0 - R@5: 73.2 - R@10: 83.2 - Median R: 2.0 - Mean R: 11.4
2023-04-24 02:15:08,369:INFO: Video-to-Text:
2023-04-24 02:15:08,370:INFO: 	>>>  V2T$R@1: 60.5 - V2T$R@5: 88.1 - V2T$R@10: 93.2 - V2T$Median R: 1.0 - V2T$Mean R: 3.9
2023-04-24 10:32:11,478:INFO: device: cuda:0 n_gpu: 8
2023-04-24 10:32:11,761:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msvd/pytorch_model.bin.2
2023-04-24 10:32:12,140:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 10:32:12,140:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 10:32:12,140:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 10:32:12,141:WARNING: 	 embed_dim: 512
2023-04-24 10:32:12,141:WARNING: 	 image_resolution: 224
2023-04-24 10:32:12,141:WARNING: 	 vision_layers: 12
2023-04-24 10:32:12,141:WARNING: 	 vision_width: 768
2023-04-24 10:32:12,141:WARNING: 	 vision_patch_size: 32
2023-04-24 10:32:12,141:WARNING: 	 context_length: 77
2023-04-24 10:32:12,141:WARNING: 	 vocab_size: 49408
2023-04-24 10:32:12,141:WARNING: 	 transformer_width: 512
2023-04-24 10:32:12,141:WARNING: 	 transformer_heads: 8
2023-04-24 10:32:12,141:WARNING: 	 transformer_layers: 12
2023-04-24 10:32:12,141:WARNING: 	 cut_top_layer: 0
2023-04-24 10:32:13,909:WARNING: 	 sim_type: seqTransf
2023-04-24 10:32:21,050:INFO: --------------------
2023-04-24 10:32:23,945:INFO: ***** Running test *****
2023-04-24 10:32:23,945:INFO:   Num examples = 27763
2023-04-24 10:32:23,945:INFO:   Batch size = 64
2023-04-24 10:32:23,946:INFO:   Num steps = 434
2023-04-24 10:32:23,949:WARNING: Eval under the multi-sentence per video clip setting.
2023-04-24 10:32:23,949:WARNING: sentence num: 27763, video num: 670
2023-04-24 10:32:33,276:INFO: device: cuda:0 n_gpu: 8
2023-04-24 10:32:33,538:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msvd/pytorch_model.bin.2
2023-04-24 10:32:33,903:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 10:32:33,904:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 10:32:33,904:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 10:32:33,904:WARNING: 	 embed_dim: 512
2023-04-24 10:32:33,904:WARNING: 	 image_resolution: 224
2023-04-24 10:32:33,904:WARNING: 	 vision_layers: 12
2023-04-24 10:32:33,904:WARNING: 	 vision_width: 768
2023-04-24 10:32:33,904:WARNING: 	 vision_patch_size: 32
2023-04-24 10:32:33,904:WARNING: 	 context_length: 77
2023-04-24 10:32:33,904:WARNING: 	 vocab_size: 49408
2023-04-24 10:32:33,904:WARNING: 	 transformer_width: 512
2023-04-24 10:32:33,904:WARNING: 	 transformer_heads: 8
2023-04-24 10:32:33,904:WARNING: 	 transformer_layers: 12
2023-04-24 10:32:33,904:WARNING: 	 cut_top_layer: 0
2023-04-24 10:32:36,877:WARNING: 	 sim_type: seqTransf
2023-04-24 10:32:45,072:INFO: --------------------
2023-04-24 10:32:47,770:INFO: ***** Running test *****
2023-04-24 10:32:47,770:INFO:   Num examples = 27763
2023-04-24 10:32:47,770:INFO:   Batch size = 64
2023-04-24 10:32:47,770:INFO:   Num steps = 434
2023-04-24 10:32:47,773:WARNING: Eval under the multi-sentence per video clip setting.
2023-04-24 10:32:47,773:WARNING: sentence num: 27763, video num: 670
2023-04-24 11:23:26,106:INFO: before reshape, sim matrix size: 27763 x 670
2023-04-24 11:23:26,226:INFO: after reshape, sim matrix size: 670 x 81 x 670
2023-04-24 11:23:28,255:INFO: Text-to-Video:
2023-04-24 11:23:28,255:INFO: 	>>>  R@1: 43.0 - R@5: 73.2 - R@10: 83.2 - Median R: 2.0 - Mean R: 11.4
2023-04-24 11:23:28,255:INFO: Video-to-Text:
2023-04-24 11:23:28,255:INFO: 	>>>  V2T$R@1: 60.5 - V2T$R@5: 88.1 - V2T$R@10: 93.2 - V2T$Median R: 1.0 - V2T$Mean R: 3.9
2023-04-24 11:24:17,231:INFO: before reshape, sim matrix size: 27763 x 670
2023-04-24 11:24:17,364:INFO: after reshape, sim matrix size: 670 x 81 x 670
2023-04-24 11:24:19,337:INFO: Text-to-Video:
2023-04-24 11:24:19,338:INFO: 	>>>  R@1: 0.4 - R@5: 2.1 - R@10: 3.8 - Median R: 254.0 - Mean R: 277.9
2023-04-24 11:24:19,338:INFO: Video-to-Text:
2023-04-24 11:24:19,338:INFO: 	>>>  V2T$R@1: 0.3 - V2T$R@5: 1.9 - V2T$R@10: 4.4 - V2T$Median R: 128.0 - V2T$Mean R: 159.8
2023-04-24 12:22:44,563:INFO: device: cuda:0 n_gpu: 8
2023-04-24 12:22:44,846:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msvd/pytorch_model.bin.2
2023-04-24 12:22:45,223:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-24 12:22:45,224:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-24 12:22:45,224:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-24 12:22:45,224:WARNING: 	 embed_dim: 512
2023-04-24 12:22:45,224:WARNING: 	 image_resolution: 224
2023-04-24 12:22:45,224:WARNING: 	 vision_layers: 12
2023-04-24 12:22:45,224:WARNING: 	 vision_width: 768
2023-04-24 12:22:45,224:WARNING: 	 vision_patch_size: 32
2023-04-24 12:22:45,224:WARNING: 	 context_length: 77
2023-04-24 12:22:45,224:WARNING: 	 vocab_size: 49408
2023-04-24 12:22:45,224:WARNING: 	 transformer_width: 512
2023-04-24 12:22:45,224:WARNING: 	 transformer_heads: 8
2023-04-24 12:22:45,224:WARNING: 	 transformer_layers: 12
2023-04-24 12:22:45,224:WARNING: 	 cut_top_layer: 0
2023-04-24 12:22:46,955:WARNING: 	 sim_type: seqTransf
2023-04-24 12:22:53,914:INFO: --------------------
2023-04-24 12:22:56,632:INFO: ***** Running test *****
2023-04-24 12:22:56,632:INFO:   Num examples = 27763
2023-04-24 12:22:56,632:INFO:   Batch size = 64
2023-04-24 12:22:56,632:INFO:   Num steps = 434
2023-04-24 12:22:56,635:WARNING: Eval under the multi-sentence per video clip setting.
2023-04-24 12:22:56,636:WARNING: sentence num: 27763, video num: 670
2023-04-24 13:13:30,837:INFO: before reshape, sim matrix size: 27763 x 670
2023-04-24 13:13:30,957:INFO: after reshape, sim matrix size: 670 x 81 x 670
2023-04-24 13:13:32,897:INFO: Text-to-Video:
2023-04-24 13:13:32,897:INFO: 	>>>  R@1: 0.4 - R@5: 1.7 - R@10: 3.2 - Median R: 267.0 - Mean R: 285.8
2023-04-24 13:13:32,897:INFO: Video-to-Text:
2023-04-24 13:13:32,897:INFO: 	>>>  V2T$R@1: 0.9 - V2T$R@5: 5.1 - V2T$R@10: 8.4 - V2T$Median R: 129.0 - V2T$Mean R: 179.3
2023-04-25 00:22:34,677:INFO: device: cuda:0 n_gpu: 8
2023-04-25 00:22:35,467:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2
2023-04-25 00:22:36,034:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-25 00:22:36,035:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-25 00:22:36,035:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-25 00:22:36,035:WARNING: 	 embed_dim: 512
2023-04-25 00:22:36,035:WARNING: 	 image_resolution: 224
2023-04-25 00:22:36,035:WARNING: 	 vision_layers: 12
2023-04-25 00:22:36,036:WARNING: 	 vision_width: 768
2023-04-25 00:22:36,036:WARNING: 	 vision_patch_size: 32
2023-04-25 00:22:36,036:WARNING: 	 context_length: 77
2023-04-25 00:22:36,036:WARNING: 	 vocab_size: 49408
2023-04-25 00:22:36,036:WARNING: 	 transformer_width: 512
2023-04-25 00:22:36,036:WARNING: 	 transformer_heads: 8
2023-04-25 00:22:36,036:WARNING: 	 transformer_layers: 12
2023-04-25 00:22:36,036:WARNING: 	 cut_top_layer: 0
2023-04-25 00:22:39,655:WARNING: 	 sim_type: seqTransf
2023-04-25 00:22:48,734:INFO: --------------------
2023-04-25 00:22:51,550:INFO: ***** Running test *****
2023-04-25 00:22:51,550:INFO:   Num examples = 1000
2023-04-25 00:22:51,550:INFO:   Batch size = 64
2023-04-25 00:22:51,550:INFO:   Num steps = 16
2023-04-25 00:23:40,963:INFO: sim matrix size: 1000, 1000
2023-04-25 00:23:41,099:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-25 00:23:41,099:INFO: Text-to-Video:
2023-04-25 00:23:41,100:INFO: 	>>>  R@1: 46.0 - R@5: 71.6 - R@10: 81.6 - Median R: 2.0 - Mean R: 14.5
2023-04-25 00:23:41,100:INFO: Video-to-Text:
2023-04-25 00:23:41,100:INFO: 	>>>  V2T$R@1: 43.8 - V2T$R@5: 72.7 - V2T$R@10: 82.5 - V2T$Median R: 2.0 - V2T$Mean R: 10.2
2023-04-25 00:25:47,758:INFO: device: cuda:0 n_gpu: 8
2023-04-25 00:25:48,071:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2
2023-04-25 00:25:48,506:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-25 00:25:48,507:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-25 00:25:48,507:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-25 00:25:48,508:WARNING: 	 embed_dim: 512
2023-04-25 00:25:48,508:WARNING: 	 image_resolution: 224
2023-04-25 00:25:48,508:WARNING: 	 vision_layers: 12
2023-04-25 00:25:48,508:WARNING: 	 vision_width: 768
2023-04-25 00:25:48,508:WARNING: 	 vision_patch_size: 32
2023-04-25 00:25:48,508:WARNING: 	 context_length: 77
2023-04-25 00:25:48,508:WARNING: 	 vocab_size: 49408
2023-04-25 00:25:48,508:WARNING: 	 transformer_width: 512
2023-04-25 00:25:48,508:WARNING: 	 transformer_heads: 8
2023-04-25 00:25:48,508:WARNING: 	 transformer_layers: 12
2023-04-25 00:25:48,508:WARNING: 	 cut_top_layer: 0
2023-04-25 00:25:52,098:WARNING: 	 sim_type: seqTransf
2023-04-25 00:26:01,603:INFO: --------------------
2023-04-25 00:26:04,368:INFO: ***** Running test *****
2023-04-25 00:26:04,369:INFO:   Num examples = 1000
2023-04-25 00:26:04,369:INFO:   Batch size = 64
2023-04-25 00:26:04,369:INFO:   Num steps = 16
2023-04-25 00:26:48,935:INFO: sim matrix size: 1000, 1000
2023-04-25 00:26:49,066:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-25 00:26:49,066:INFO: Text-to-Video:
2023-04-25 00:26:49,066:INFO: 	>>>  R@1: 37.8 - R@5: 65.0 - R@10: 76.1 - Median R: 3.0 - Mean R: 19.3
2023-04-25 00:26:49,066:INFO: Video-to-Text:
2023-04-25 00:26:49,066:INFO: 	>>>  V2T$R@1: 37.9 - V2T$R@5: 65.0 - V2T$R@10: 75.8 - V2T$Median R: 3.0 - V2T$Mean R: 15.4
2023-04-25 00:28:05,842:INFO: device: cuda:0 n_gpu: 8
2023-04-25 00:28:06,156:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2
2023-04-25 00:28:06,590:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-25 00:28:06,591:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-25 00:28:06,591:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-25 00:28:06,591:WARNING: 	 embed_dim: 512
2023-04-25 00:28:06,591:WARNING: 	 image_resolution: 224
2023-04-25 00:28:06,591:WARNING: 	 vision_layers: 12
2023-04-25 00:28:06,591:WARNING: 	 vision_width: 768
2023-04-25 00:28:06,591:WARNING: 	 vision_patch_size: 32
2023-04-25 00:28:06,591:WARNING: 	 context_length: 77
2023-04-25 00:28:06,591:WARNING: 	 vocab_size: 49408
2023-04-25 00:28:06,591:WARNING: 	 transformer_width: 512
2023-04-25 00:28:06,591:WARNING: 	 transformer_heads: 8
2023-04-25 00:28:06,591:WARNING: 	 transformer_layers: 12
2023-04-25 00:28:06,591:WARNING: 	 cut_top_layer: 0
2023-04-25 00:28:10,582:WARNING: 	 sim_type: seqTransf
2023-04-25 00:28:20,253:INFO: --------------------
2023-04-25 00:28:22,903:INFO: ***** Running test *****
2023-04-25 00:28:22,903:INFO:   Num examples = 1000
2023-04-25 00:28:22,903:INFO:   Batch size = 64
2023-04-25 00:28:22,903:INFO:   Num steps = 16
2023-04-25 00:29:09,378:INFO: sim matrix size: 1000, 1000
2023-04-25 00:29:09,490:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-25 00:29:09,491:INFO: Text-to-Video:
2023-04-25 00:29:09,491:INFO: 	>>>  R@1: 34.0 - R@5: 61.8 - R@10: 72.7 - Median R: 3.0 - Mean R: 19.5
2023-04-25 00:29:09,491:INFO: Video-to-Text:
2023-04-25 00:29:09,491:INFO: 	>>>  V2T$R@1: 37.4 - V2T$R@5: 63.2 - V2T$R@10: 74.2 - V2T$Median R: 3.0 - V2T$Mean R: 16.9
2023-04-25 00:30:09,148:INFO: device: cuda:0 n_gpu: 8
2023-04-25 00:30:09,454:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2
2023-04-25 00:30:09,868:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-25 00:30:09,869:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-25 00:30:09,869:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-25 00:30:09,870:WARNING: 	 embed_dim: 512
2023-04-25 00:30:09,870:WARNING: 	 image_resolution: 224
2023-04-25 00:30:09,870:WARNING: 	 vision_layers: 12
2023-04-25 00:30:09,870:WARNING: 	 vision_width: 768
2023-04-25 00:30:09,870:WARNING: 	 vision_patch_size: 32
2023-04-25 00:30:09,870:WARNING: 	 context_length: 77
2023-04-25 00:30:09,870:WARNING: 	 vocab_size: 49408
2023-04-25 00:30:09,870:WARNING: 	 transformer_width: 512
2023-04-25 00:30:09,870:WARNING: 	 transformer_heads: 8
2023-04-25 00:30:09,870:WARNING: 	 transformer_layers: 12
2023-04-25 00:30:09,871:WARNING: 	 cut_top_layer: 0
2023-04-25 00:30:13,526:WARNING: 	 sim_type: seqTransf
2023-04-25 00:30:22,448:INFO: --------------------
2023-04-25 00:30:24,983:INFO: ***** Running test *****
2023-04-25 00:30:24,983:INFO:   Num examples = 1000
2023-04-25 00:30:24,983:INFO:   Batch size = 64
2023-04-25 00:30:24,983:INFO:   Num steps = 16
2023-04-25 00:31:10,919:INFO: sim matrix size: 1000, 1000
2023-04-25 00:31:11,041:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-25 00:31:11,041:INFO: Text-to-Video:
2023-04-25 00:31:11,041:INFO: 	>>>  R@1: 44.0 - R@5: 72.1 - R@10: 80.4 - Median R: 2.0 - Mean R: 15.8
2023-04-25 00:31:11,041:INFO: Video-to-Text:
2023-04-25 00:31:11,041:INFO: 	>>>  V2T$R@1: 43.0 - V2T$R@5: 72.2 - V2T$R@10: 81.5 - V2T$Median R: 2.0 - V2T$Mean R: 10.4
2023-04-25 00:34:21,317:INFO: device: cuda:0 n_gpu: 8
2023-04-25 00:34:21,618:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2
2023-04-25 00:34:22,031:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-25 00:34:22,031:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-25 00:34:22,031:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-25 00:34:22,031:WARNING: 	 embed_dim: 512
2023-04-25 00:34:22,032:WARNING: 	 image_resolution: 224
2023-04-25 00:34:22,032:WARNING: 	 vision_layers: 12
2023-04-25 00:34:22,032:WARNING: 	 vision_width: 768
2023-04-25 00:34:22,032:WARNING: 	 vision_patch_size: 32
2023-04-25 00:34:22,032:WARNING: 	 context_length: 77
2023-04-25 00:34:22,032:WARNING: 	 vocab_size: 49408
2023-04-25 00:34:22,032:WARNING: 	 transformer_width: 512
2023-04-25 00:34:22,032:WARNING: 	 transformer_heads: 8
2023-04-25 00:34:22,032:WARNING: 	 transformer_layers: 12
2023-04-25 00:34:22,032:WARNING: 	 cut_top_layer: 0
2023-04-25 00:34:25,592:WARNING: 	 sim_type: seqTransf
2023-04-25 00:34:35,090:INFO: --------------------
2023-04-25 00:34:37,971:INFO: ***** Running test *****
2023-04-25 00:34:37,971:INFO:   Num examples = 1000
2023-04-25 00:34:37,971:INFO:   Batch size = 64
2023-04-25 00:34:37,971:INFO:   Num steps = 16
2023-04-25 00:35:24,527:INFO: sim matrix size: 1000, 1000
2023-04-25 00:35:24,634:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-25 00:35:24,634:INFO: Text-to-Video:
2023-04-25 00:35:24,634:INFO: 	>>>  R@1: 38.8 - R@5: 64.8 - R@10: 75.4 - Median R: 2.0 - Mean R: 21.1
2023-04-25 00:35:24,634:INFO: Video-to-Text:
2023-04-25 00:35:24,634:INFO: 	>>>  V2T$R@1: 38.0 - V2T$R@5: 66.1 - V2T$R@10: 76.2 - V2T$Median R: 3.0 - V2T$Mean R: 14.1
2023-04-25 00:37:01,871:INFO: device: cuda:0 n_gpu: 8
2023-04-25 00:37:02,165:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2
2023-04-25 00:37:02,571:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-25 00:37:02,572:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-25 00:37:02,572:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-25 00:37:02,573:WARNING: 	 embed_dim: 512
2023-04-25 00:37:02,573:WARNING: 	 image_resolution: 224
2023-04-25 00:37:02,573:WARNING: 	 vision_layers: 12
2023-04-25 00:37:02,573:WARNING: 	 vision_width: 768
2023-04-25 00:37:02,573:WARNING: 	 vision_patch_size: 32
2023-04-25 00:37:02,573:WARNING: 	 context_length: 77
2023-04-25 00:37:02,573:WARNING: 	 vocab_size: 49408
2023-04-25 00:37:02,573:WARNING: 	 transformer_width: 512
2023-04-25 00:37:02,573:WARNING: 	 transformer_heads: 8
2023-04-25 00:37:02,573:WARNING: 	 transformer_layers: 12
2023-04-25 00:37:02,573:WARNING: 	 cut_top_layer: 0
2023-04-25 00:37:05,886:WARNING: 	 sim_type: seqTransf
2023-04-25 00:37:14,535:INFO: --------------------
2023-04-25 00:37:17,118:INFO: ***** Running test *****
2023-04-25 00:37:17,118:INFO:   Num examples = 1000
2023-04-25 00:37:17,118:INFO:   Batch size = 64
2023-04-25 00:37:17,118:INFO:   Num steps = 16
2023-04-25 00:38:04,685:INFO: sim matrix size: 1000, 1000
2023-04-25 00:38:04,793:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-25 00:38:04,793:INFO: Text-to-Video:
2023-04-25 00:38:04,793:INFO: 	>>>  R@1: 8.4 - R@5: 21.9 - R@10: 28.7 - Median R: 46.5 - Mean R: 149.8
2023-04-25 00:38:04,793:INFO: Video-to-Text:
2023-04-25 00:38:04,793:INFO: 	>>>  V2T$R@1: 10.0 - V2T$R@5: 22.5 - V2T$R@10: 28.1 - V2T$Median R: 52.0 - V2T$Mean R: 124.9
2023-04-25 00:38:51,345:INFO: device: cuda:0 n_gpu: 8
2023-04-25 00:38:51,641:INFO: Model loaded from /playpen-storage/avinashm/Experiments/compositionality/weights/clip2video/msrvtt/pytorch_model.bin.2
2023-04-25 00:38:52,040:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-25 00:38:52,041:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-25 00:38:52,041:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-25 00:38:52,041:WARNING: 	 embed_dim: 512
2023-04-25 00:38:52,041:WARNING: 	 image_resolution: 224
2023-04-25 00:38:52,041:WARNING: 	 vision_layers: 12
2023-04-25 00:38:52,041:WARNING: 	 vision_width: 768
2023-04-25 00:38:52,041:WARNING: 	 vision_patch_size: 32
2023-04-25 00:38:52,041:WARNING: 	 context_length: 77
2023-04-25 00:38:52,041:WARNING: 	 vocab_size: 49408
2023-04-25 00:38:52,041:WARNING: 	 transformer_width: 512
2023-04-25 00:38:52,042:WARNING: 	 transformer_heads: 8
2023-04-25 00:38:52,042:WARNING: 	 transformer_layers: 12
2023-04-25 00:38:52,042:WARNING: 	 cut_top_layer: 0
2023-04-25 00:38:55,697:WARNING: 	 sim_type: seqTransf
2023-04-25 00:39:04,663:INFO: --------------------
2023-04-25 00:39:08,017:INFO: ***** Running test *****
2023-04-25 00:39:08,017:INFO:   Num examples = 1000
2023-04-25 00:39:08,017:INFO:   Batch size = 64
2023-04-25 00:39:08,018:INFO:   Num steps = 16
2023-04-25 00:39:58,445:INFO: sim matrix size: 1000, 1000
2023-04-25 00:39:58,551:INFO: 	 Length-T: 1000, Length-V:1000
2023-04-25 00:39:58,551:INFO: Text-to-Video:
2023-04-25 00:39:58,551:INFO: 	>>>  R@1: 35.3 - R@5: 61.7 - R@10: 72.2 - Median R: 3.0 - Mean R: 28.2
2023-04-25 00:39:58,551:INFO: Video-to-Text:
2023-04-25 00:39:58,551:INFO: 	>>>  V2T$R@1: 34.9 - V2T$R@5: 61.1 - V2T$R@10: 71.7 - V2T$Median R: 3.0 - V2T$Mean R: 19.6
2023-04-27 11:10:56,819:INFO: device: cuda:0 n_gpu: 8
2023-04-27 11:10:58,077:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-04-27 11:10:59,221:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-27 11:10:59,222:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-27 11:10:59,222:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-27 11:10:59,222:WARNING: 	 embed_dim: 512
2023-04-27 11:10:59,222:WARNING: 	 image_resolution: 224
2023-04-27 11:10:59,222:WARNING: 	 vision_layers: 12
2023-04-27 11:10:59,222:WARNING: 	 vision_width: 768
2023-04-27 11:10:59,222:WARNING: 	 vision_patch_size: 32
2023-04-27 11:10:59,222:WARNING: 	 context_length: 77
2023-04-27 11:10:59,222:WARNING: 	 vocab_size: 49408
2023-04-27 11:10:59,222:WARNING: 	 transformer_width: 512
2023-04-27 11:10:59,222:WARNING: 	 transformer_heads: 8
2023-04-27 11:10:59,222:WARNING: 	 transformer_layers: 12
2023-04-27 11:10:59,222:WARNING: 	 cut_top_layer: 0
2023-04-27 11:11:00,938:WARNING: 	 sim_type: seqTransf
2023-04-27 11:11:07,918:INFO: --------------------
2023-04-27 11:12:09,937:INFO: device: cuda:0 n_gpu: 8
2023-04-27 11:12:10,201:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-04-27 11:12:10,587:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-27 11:12:10,587:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-27 11:12:10,587:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-27 11:12:10,587:WARNING: 	 embed_dim: 512
2023-04-27 11:12:10,588:WARNING: 	 image_resolution: 224
2023-04-27 11:12:10,588:WARNING: 	 vision_layers: 12
2023-04-27 11:12:10,588:WARNING: 	 vision_width: 768
2023-04-27 11:12:10,588:WARNING: 	 vision_patch_size: 32
2023-04-27 11:12:10,588:WARNING: 	 context_length: 77
2023-04-27 11:12:10,588:WARNING: 	 vocab_size: 49408
2023-04-27 11:12:10,588:WARNING: 	 transformer_width: 512
2023-04-27 11:12:10,588:WARNING: 	 transformer_heads: 8
2023-04-27 11:12:10,588:WARNING: 	 transformer_layers: 12
2023-04-27 11:12:10,588:WARNING: 	 cut_top_layer: 0
2023-04-27 11:12:12,387:WARNING: 	 sim_type: seqTransf
2023-04-27 11:12:19,445:INFO: --------------------
2023-04-28 01:54:14,349:INFO: device: cuda:0 n_gpu: 8
2023-04-28 01:54:14,622:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-04-28 01:54:15,017:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-28 01:54:15,017:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-28 01:54:15,017:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-28 01:54:15,018:WARNING: 	 embed_dim: 512
2023-04-28 01:54:15,018:WARNING: 	 image_resolution: 224
2023-04-28 01:54:15,018:WARNING: 	 vision_layers: 12
2023-04-28 01:54:15,018:WARNING: 	 vision_width: 768
2023-04-28 01:54:15,018:WARNING: 	 vision_patch_size: 32
2023-04-28 01:54:15,018:WARNING: 	 context_length: 77
2023-04-28 01:54:15,018:WARNING: 	 vocab_size: 49408
2023-04-28 01:54:15,018:WARNING: 	 transformer_width: 512
2023-04-28 01:54:15,018:WARNING: 	 transformer_heads: 8
2023-04-28 01:54:15,018:WARNING: 	 transformer_layers: 12
2023-04-28 01:54:15,018:WARNING: 	 cut_top_layer: 0
2023-04-28 01:54:16,518:WARNING: 	 sim_type: seqTransf
2023-04-28 01:54:23,565:INFO: --------------------
2023-04-28 01:54:59,972:INFO: device: cuda:0 n_gpu: 8
2023-04-28 01:55:00,245:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-04-28 01:55:00,628:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-28 01:55:00,628:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-28 01:55:00,628:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-28 01:55:00,628:WARNING: 	 embed_dim: 512
2023-04-28 01:55:00,629:WARNING: 	 image_resolution: 224
2023-04-28 01:55:00,629:WARNING: 	 vision_layers: 12
2023-04-28 01:55:00,629:WARNING: 	 vision_width: 768
2023-04-28 01:55:00,629:WARNING: 	 vision_patch_size: 32
2023-04-28 01:55:00,629:WARNING: 	 context_length: 77
2023-04-28 01:55:00,629:WARNING: 	 vocab_size: 49408
2023-04-28 01:55:00,629:WARNING: 	 transformer_width: 512
2023-04-28 01:55:00,629:WARNING: 	 transformer_heads: 8
2023-04-28 01:55:00,629:WARNING: 	 transformer_layers: 12
2023-04-28 01:55:00,629:WARNING: 	 cut_top_layer: 0
2023-04-28 01:55:02,363:WARNING: 	 sim_type: seqTransf
2023-04-28 01:55:09,224:INFO: --------------------
2023-04-28 02:01:56,470:INFO: device: cuda:0 n_gpu: 8
2023-04-28 02:01:56,742:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-04-28 02:01:57,116:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-28 02:01:57,116:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-28 02:01:57,116:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-28 02:01:57,116:WARNING: 	 embed_dim: 512
2023-04-28 02:01:57,117:WARNING: 	 image_resolution: 224
2023-04-28 02:01:57,117:WARNING: 	 vision_layers: 12
2023-04-28 02:01:57,117:WARNING: 	 vision_width: 768
2023-04-28 02:01:57,117:WARNING: 	 vision_patch_size: 32
2023-04-28 02:01:57,117:WARNING: 	 context_length: 77
2023-04-28 02:01:57,117:WARNING: 	 vocab_size: 49408
2023-04-28 02:01:57,117:WARNING: 	 transformer_width: 512
2023-04-28 02:01:57,117:WARNING: 	 transformer_heads: 8
2023-04-28 02:01:57,117:WARNING: 	 transformer_layers: 12
2023-04-28 02:01:57,117:WARNING: 	 cut_top_layer: 0
2023-04-28 02:01:58,801:WARNING: 	 sim_type: seqTransf
2023-04-28 02:02:05,883:INFO: --------------------
2023-04-28 02:02:08,586:INFO: ***** Running test *****
2023-04-28 02:02:08,586:INFO:   Num examples = 27763
2023-04-28 02:02:08,586:INFO:   Batch size = 64
2023-04-28 02:02:08,586:INFO:   Num steps = 434
2023-04-28 02:02:08,589:WARNING: Eval under the multi-sentence per video clip setting.
2023-04-28 02:02:08,590:WARNING: sentence num: 27763, video num: 670
2023-04-28 02:02:53,303:INFO: device: cuda:0 n_gpu: 8
2023-04-28 02:02:53,580:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-04-28 02:02:53,965:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-28 02:02:53,965:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-28 02:02:53,965:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-28 02:02:53,966:WARNING: 	 embed_dim: 512
2023-04-28 02:02:53,966:WARNING: 	 image_resolution: 224
2023-04-28 02:02:53,966:WARNING: 	 vision_layers: 12
2023-04-28 02:02:53,966:WARNING: 	 vision_width: 768
2023-04-28 02:02:53,966:WARNING: 	 vision_patch_size: 32
2023-04-28 02:02:53,966:WARNING: 	 context_length: 77
2023-04-28 02:02:53,966:WARNING: 	 vocab_size: 49408
2023-04-28 02:02:53,966:WARNING: 	 transformer_width: 512
2023-04-28 02:02:53,966:WARNING: 	 transformer_heads: 8
2023-04-28 02:02:53,966:WARNING: 	 transformer_layers: 12
2023-04-28 02:02:53,966:WARNING: 	 cut_top_layer: 0
2023-04-28 02:02:55,513:WARNING: 	 sim_type: seqTransf
2023-04-28 02:03:02,438:INFO: --------------------
2023-04-28 02:03:05,174:INFO: ***** Running test *****
2023-04-28 02:03:05,175:INFO:   Num examples = 27763
2023-04-28 02:03:05,175:INFO:   Batch size = 64
2023-04-28 02:03:05,175:INFO:   Num steps = 434
2023-04-28 02:03:05,178:WARNING: Eval under the multi-sentence per video clip setting.
2023-04-28 02:03:05,178:WARNING: sentence num: 27763, video num: 670
2023-04-28 02:03:52,074:INFO: device: cuda:0 n_gpu: 8
2023-04-28 02:03:52,342:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-04-28 02:03:52,710:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-04-28 02:03:52,710:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-04-28 02:03:52,710:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-04-28 02:03:52,711:WARNING: 	 embed_dim: 512
2023-04-28 02:03:52,711:WARNING: 	 image_resolution: 224
2023-04-28 02:03:52,711:WARNING: 	 vision_layers: 12
2023-04-28 02:03:52,711:WARNING: 	 vision_width: 768
2023-04-28 02:03:52,711:WARNING: 	 vision_patch_size: 32
2023-04-28 02:03:52,711:WARNING: 	 context_length: 77
2023-04-28 02:03:52,711:WARNING: 	 vocab_size: 49408
2023-04-28 02:03:52,711:WARNING: 	 transformer_width: 512
2023-04-28 02:03:52,711:WARNING: 	 transformer_heads: 8
2023-04-28 02:03:52,711:WARNING: 	 transformer_layers: 12
2023-04-28 02:03:52,711:WARNING: 	 cut_top_layer: 0
2023-04-28 02:03:55,561:WARNING: 	 sim_type: seqTransf
2023-04-28 02:04:03,750:INFO: --------------------
2023-04-28 02:04:06,615:INFO: ***** Running test *****
2023-04-28 02:04:06,615:INFO:   Num examples = 27763
2023-04-28 02:04:06,615:INFO:   Batch size = 64
2023-04-28 02:04:06,615:INFO:   Num steps = 434
2023-04-28 02:04:06,619:WARNING: Eval under the multi-sentence per video clip setting.
2023-04-28 02:04:06,619:WARNING: sentence num: 27763, video num: 670
2023-04-28 02:53:46,729:INFO: before reshape, sim matrix size: 27763 x 670
2023-04-28 02:53:46,851:INFO: after reshape, sim matrix size: 670 x 81 x 670
2023-04-28 02:53:48,933:INFO: Text-to-Video:
2023-04-28 02:53:48,933:INFO: 	>>>  R@1: 0.2 - R@5: 0.9 - R@10: 1.8 - Median R: 315.0 - Mean R: 319.3
2023-04-28 02:53:48,933:INFO: Video-to-Text:
2023-04-28 02:53:48,933:INFO: 	>>>  V2T$R@1: 0.1 - V2T$R@5: 0.8 - V2T$R@10: 1.7 - V2T$Median R: 167.0 - V2T$Mean R: 193.6
2023-04-28 02:55:38,274:INFO: before reshape, sim matrix size: 27763 x 670
2023-04-28 02:55:38,422:INFO: after reshape, sim matrix size: 670 x 81 x 670
2023-04-28 02:55:40,382:INFO: Text-to-Video:
2023-04-28 02:55:40,383:INFO: 	>>>  R@1: 0.3 - R@5: 1.3 - R@10: 2.6 - Median R: 283.0 - Mean R: 296.5
2023-04-28 02:55:40,383:INFO: Video-to-Text:
2023-04-28 02:55:40,383:INFO: 	>>>  V2T$R@1: 1.1 - V2T$R@5: 3.9 - V2T$R@10: 8.0 - V2T$Median R: 117.0 - V2T$Mean R: 182.5
2023-05-15 00:41:14,439:INFO: device: cuda:0 n_gpu: 2
2023-05-15 00:41:15,434:INFO: Model loaded from ../weights/clip2video/msrvtt/pytorch_model.bin.2
2023-05-15 00:41:15,910:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-15 00:41:15,911:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-15 00:41:15,911:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-15 00:41:15,912:WARNING: 	 embed_dim: 512
2023-05-15 00:41:15,912:WARNING: 	 image_resolution: 224
2023-05-15 00:41:15,912:WARNING: 	 vision_layers: 12
2023-05-15 00:41:15,912:WARNING: 	 vision_width: 768
2023-05-15 00:41:15,913:WARNING: 	 vision_patch_size: 32
2023-05-15 00:41:15,913:WARNING: 	 context_length: 77
2023-05-15 00:41:15,913:WARNING: 	 vocab_size: 49408
2023-05-15 00:41:15,913:WARNING: 	 transformer_width: 512
2023-05-15 00:41:15,913:WARNING: 	 transformer_heads: 8
2023-05-15 00:41:15,913:WARNING: 	 transformer_layers: 12
2023-05-15 00:41:15,913:WARNING: 	 cut_top_layer: 0
2023-05-15 00:41:20,167:WARNING: 	 sim_type: seqTransf
2023-05-15 00:41:30,466:INFO: --------------------
2023-05-15 00:52:52,412:INFO: device: cuda:0 n_gpu: 2
2023-05-15 00:52:52,729:INFO: Model loaded from ../weights/clip2video/msrvtt/pytorch_model.bin.2
2023-05-15 00:52:53,160:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-15 00:52:53,160:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-15 00:52:53,161:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-15 00:52:53,161:WARNING: 	 embed_dim: 512
2023-05-15 00:52:53,161:WARNING: 	 image_resolution: 224
2023-05-15 00:52:53,161:WARNING: 	 vision_layers: 12
2023-05-15 00:52:53,161:WARNING: 	 vision_width: 768
2023-05-15 00:52:53,161:WARNING: 	 vision_patch_size: 32
2023-05-15 00:52:53,162:WARNING: 	 context_length: 77
2023-05-15 00:52:53,162:WARNING: 	 vocab_size: 49408
2023-05-15 00:52:53,162:WARNING: 	 transformer_width: 512
2023-05-15 00:52:53,162:WARNING: 	 transformer_heads: 8
2023-05-15 00:52:53,162:WARNING: 	 transformer_layers: 12
2023-05-15 00:52:53,162:WARNING: 	 cut_top_layer: 0
2023-05-15 00:52:57,120:WARNING: 	 sim_type: seqTransf
2023-05-15 00:53:06,854:INFO: --------------------
2023-05-15 00:53:10,414:INFO: ***** Running test *****
2023-05-15 00:53:10,414:INFO:   Num examples = 1000
2023-05-15 00:53:10,414:INFO:   Batch size = 64
2023-05-15 00:53:10,414:INFO:   Num steps = 16
2023-05-15 00:56:49,244:INFO: device: cuda:0 n_gpu: 2
2023-05-15 00:56:49,556:INFO: Model loaded from ../weights/clip2video/msrvtt/pytorch_model.bin.2
2023-05-15 00:56:49,978:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-15 00:56:49,979:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-15 00:56:49,979:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-15 00:56:49,980:WARNING: 	 embed_dim: 512
2023-05-15 00:56:49,980:WARNING: 	 image_resolution: 224
2023-05-15 00:56:49,981:WARNING: 	 vision_layers: 12
2023-05-15 00:56:49,981:WARNING: 	 vision_width: 768
2023-05-15 00:56:49,981:WARNING: 	 vision_patch_size: 32
2023-05-15 00:56:49,981:WARNING: 	 context_length: 77
2023-05-15 00:56:49,981:WARNING: 	 vocab_size: 49408
2023-05-15 00:56:49,981:WARNING: 	 transformer_width: 512
2023-05-15 00:56:49,981:WARNING: 	 transformer_heads: 8
2023-05-15 00:56:49,981:WARNING: 	 transformer_layers: 12
2023-05-15 00:56:49,981:WARNING: 	 cut_top_layer: 0
2023-05-15 00:56:53,760:WARNING: 	 sim_type: seqTransf
2023-05-15 00:57:03,895:INFO: --------------------
2023-05-15 00:57:07,606:INFO: ***** Running test *****
2023-05-15 00:57:07,606:INFO:   Num examples = 1000
2023-05-15 00:57:07,606:INFO:   Batch size = 64
2023-05-15 00:57:07,606:INFO:   Num steps = 16
2023-05-15 00:59:22,958:INFO: device: cuda:0 n_gpu: 2
2023-05-15 00:59:23,288:INFO: Model loaded from ../weights/clip2video/msrvtt/pytorch_model.bin.2
2023-05-15 00:59:23,728:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-15 00:59:23,728:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-15 00:59:23,728:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-15 00:59:23,729:WARNING: 	 embed_dim: 512
2023-05-15 00:59:23,729:WARNING: 	 image_resolution: 224
2023-05-15 00:59:23,730:WARNING: 	 vision_layers: 12
2023-05-15 00:59:23,730:WARNING: 	 vision_width: 768
2023-05-15 00:59:23,730:WARNING: 	 vision_patch_size: 32
2023-05-15 00:59:23,730:WARNING: 	 context_length: 77
2023-05-15 00:59:23,730:WARNING: 	 vocab_size: 49408
2023-05-15 00:59:23,731:WARNING: 	 transformer_width: 512
2023-05-15 00:59:23,731:WARNING: 	 transformer_heads: 8
2023-05-15 00:59:23,731:WARNING: 	 transformer_layers: 12
2023-05-15 00:59:23,731:WARNING: 	 cut_top_layer: 0
2023-05-15 00:59:27,794:WARNING: 	 sim_type: seqTransf
2023-05-15 00:59:37,909:INFO: --------------------
2023-05-15 00:59:41,699:INFO: ***** Running test *****
2023-05-15 00:59:41,699:INFO:   Num examples = 1000
2023-05-15 00:59:41,699:INFO:   Batch size = 64
2023-05-15 00:59:41,699:INFO:   Num steps = 16
2023-05-15 01:03:56,658:INFO: device: cuda:0 n_gpu: 2
2023-05-15 01:03:56,971:INFO: Model loaded from ../weights/clip2video/msrvtt/pytorch_model.bin.2
2023-05-15 01:03:57,431:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-15 01:03:57,432:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-15 01:03:57,434:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-15 01:03:57,434:WARNING: 	 embed_dim: 512
2023-05-15 01:03:57,434:WARNING: 	 image_resolution: 224
2023-05-15 01:03:57,435:WARNING: 	 vision_layers: 12
2023-05-15 01:03:57,435:WARNING: 	 vision_width: 768
2023-05-15 01:03:57,435:WARNING: 	 vision_patch_size: 32
2023-05-15 01:03:57,435:WARNING: 	 context_length: 77
2023-05-15 01:03:57,436:WARNING: 	 vocab_size: 49408
2023-05-15 01:03:57,436:WARNING: 	 transformer_width: 512
2023-05-15 01:03:57,436:WARNING: 	 transformer_heads: 8
2023-05-15 01:03:57,437:WARNING: 	 transformer_layers: 12
2023-05-15 01:03:57,437:WARNING: 	 cut_top_layer: 0
2023-05-15 01:04:01,117:WARNING: 	 sim_type: seqTransf
2023-05-15 01:04:10,856:INFO: --------------------
2023-05-15 01:04:14,586:INFO: ***** Running test *****
2023-05-15 01:04:14,586:INFO:   Num examples = 1000
2023-05-15 01:04:14,586:INFO:   Batch size = 64
2023-05-15 01:04:14,586:INFO:   Num steps = 16
2023-05-15 01:05:27,607:INFO: device: cuda:0 n_gpu: 2
2023-05-15 01:05:27,939:INFO: Model loaded from ../weights/clip2video/msrvtt/pytorch_model.bin.2
2023-05-15 01:05:28,363:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-15 01:05:28,363:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-15 01:05:28,363:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-15 01:05:28,364:WARNING: 	 embed_dim: 512
2023-05-15 01:05:28,364:WARNING: 	 image_resolution: 224
2023-05-15 01:05:28,364:WARNING: 	 vision_layers: 12
2023-05-15 01:05:28,365:WARNING: 	 vision_width: 768
2023-05-15 01:05:28,365:WARNING: 	 vision_patch_size: 32
2023-05-15 01:05:28,365:WARNING: 	 context_length: 77
2023-05-15 01:05:28,366:WARNING: 	 vocab_size: 49408
2023-05-15 01:05:28,366:WARNING: 	 transformer_width: 512
2023-05-15 01:05:28,366:WARNING: 	 transformer_heads: 8
2023-05-15 01:05:28,366:WARNING: 	 transformer_layers: 12
2023-05-15 01:05:28,366:WARNING: 	 cut_top_layer: 0
2023-05-15 01:05:32,195:WARNING: 	 sim_type: seqTransf
2023-05-15 01:05:41,641:INFO: --------------------
2023-05-15 01:05:44,793:INFO: ***** Running test *****
2023-05-15 01:05:44,793:INFO:   Num examples = 1000
2023-05-15 01:05:44,793:INFO:   Batch size = 64
2023-05-15 01:05:44,793:INFO:   Num steps = 16
2023-05-15 01:06:16,825:INFO: sim matrix size: 1000, 1000
2023-05-15 01:06:16,944:INFO: 	 Length-T: 1000, Length-V:1000
2023-05-15 01:06:16,944:INFO: Text-to-Video:
2023-05-15 01:06:16,944:INFO: 	>>>  R@1: 41.7 - R@5: 69.4 - R@10: 78.6 - Median R: 2.0 - Mean R: 16.8
2023-05-15 01:06:16,944:INFO: Video-to-Text:
2023-05-15 01:06:16,944:INFO: 	>>>  V2T$R@1: 40.8 - V2T$R@5: 68.8 - V2T$R@10: 78.6 - V2T$Median R: 2.0 - V2T$Mean R: 12.5
