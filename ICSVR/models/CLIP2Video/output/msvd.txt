2023-05-06 20:34:44,352:INFO: device: cuda:0 n_gpu: 1
2023-05-06 20:34:45,141:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-06 20:34:46,016:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-06 20:34:46,018:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-06 20:34:46,018:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-06 20:34:46,018:WARNING: 	 embed_dim: 512
2023-05-06 20:34:46,018:WARNING: 	 image_resolution: 224
2023-05-06 20:34:46,019:WARNING: 	 vision_layers: 12
2023-05-06 20:34:46,019:WARNING: 	 vision_width: 768
2023-05-06 20:34:46,019:WARNING: 	 vision_patch_size: 32
2023-05-06 20:34:46,019:WARNING: 	 context_length: 77
2023-05-06 20:34:46,020:WARNING: 	 vocab_size: 49408
2023-05-06 20:34:46,020:WARNING: 	 transformer_width: 512
2023-05-06 20:34:46,020:WARNING: 	 transformer_heads: 8
2023-05-06 20:34:46,020:WARNING: 	 transformer_layers: 12
2023-05-06 20:34:46,020:WARNING: 	 cut_top_layer: 0
2023-05-06 20:34:49,075:WARNING: 	 sim_type: seqTransf
2023-05-06 20:34:58,121:INFO: --------------------
2023-05-06 20:35:01,013:INFO: ***** Running test *****
2023-05-06 20:35:01,013:INFO:   Num examples = 670
2023-05-06 20:35:01,013:INFO:   Batch size = 64
2023-05-06 20:35:01,013:INFO:   Num steps = 11
2023-05-06 20:35:01,017:WARNING: Eval under the multi-sentence per video clip setting.
2023-05-06 20:35:01,017:WARNING: sentence num: 670, video num: 670
2023-05-06 20:35:43,740:INFO: device: cuda:0 n_gpu: 1
2023-05-06 20:35:44,031:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-06 20:35:44,434:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-06 20:35:44,435:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-06 20:35:44,435:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-06 20:35:44,436:WARNING: 	 embed_dim: 512
2023-05-06 20:35:44,436:WARNING: 	 image_resolution: 224
2023-05-06 20:35:44,436:WARNING: 	 vision_layers: 12
2023-05-06 20:35:44,436:WARNING: 	 vision_width: 768
2023-05-06 20:35:44,436:WARNING: 	 vision_patch_size: 32
2023-05-06 20:35:44,436:WARNING: 	 context_length: 77
2023-05-06 20:35:44,436:WARNING: 	 vocab_size: 49408
2023-05-06 20:35:44,436:WARNING: 	 transformer_width: 512
2023-05-06 20:35:44,436:WARNING: 	 transformer_heads: 8
2023-05-06 20:35:44,436:WARNING: 	 transformer_layers: 12
2023-05-06 20:35:44,436:WARNING: 	 cut_top_layer: 0
2023-05-06 20:35:47,838:WARNING: 	 sim_type: seqTransf
2023-05-06 20:35:56,645:INFO: --------------------
2023-05-06 20:36:00,185:INFO: ***** Running test *****
2023-05-06 20:36:00,185:INFO:   Num examples = 670
2023-05-06 20:36:00,185:INFO:   Batch size = 64
2023-05-06 20:36:00,185:INFO:   Num steps = 11
2023-05-06 20:36:00,189:WARNING: Eval under the multi-sentence per video clip setting.
2023-05-06 20:36:00,189:WARNING: sentence num: 670, video num: 670
2023-05-06 20:36:34,138:INFO: before reshape, sim matrix size: 670 x 670
2023-05-06 20:36:34,144:INFO: after reshape, sim matrix size: 670 x 1 x 670
2023-05-06 20:36:34,391:INFO: Text-to-Video:
2023-05-06 20:36:34,392:INFO: 	>>>  R@1: 55.8 - R@5: 84.5 - R@10: 91.8 - Median R: 1.0 - Mean R: 3.8
2023-05-06 20:36:34,392:INFO: Video-to-Text:
2023-05-06 20:36:34,392:INFO: 	>>>  V2T$R@1: 53.6 - V2T$R@5: 84.6 - V2T$R@10: 91.9 - V2T$Median R: 1.0 - V2T$Mean R: 4.4
2023-05-06 20:38:15,361:INFO: device: cuda:0 n_gpu: 1
2023-05-06 20:38:15,638:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-06 20:38:16,034:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-06 20:38:16,035:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-06 20:38:16,035:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-06 20:38:16,036:WARNING: 	 embed_dim: 512
2023-05-06 20:38:16,036:WARNING: 	 image_resolution: 224
2023-05-06 20:38:16,036:WARNING: 	 vision_layers: 12
2023-05-06 20:38:16,036:WARNING: 	 vision_width: 768
2023-05-06 20:38:16,036:WARNING: 	 vision_patch_size: 32
2023-05-06 20:38:16,036:WARNING: 	 context_length: 77
2023-05-06 20:38:16,036:WARNING: 	 vocab_size: 49408
2023-05-06 20:38:16,037:WARNING: 	 transformer_width: 512
2023-05-06 20:38:16,037:WARNING: 	 transformer_heads: 8
2023-05-06 20:38:16,037:WARNING: 	 transformer_layers: 12
2023-05-06 20:38:16,037:WARNING: 	 cut_top_layer: 0
2023-05-06 20:38:19,737:WARNING: 	 sim_type: seqTransf
2023-05-06 20:38:28,832:INFO: --------------------
2023-05-06 20:38:32,070:INFO: ***** Running test *****
2023-05-06 20:38:32,070:INFO:   Num examples = 670
2023-05-06 20:38:32,070:INFO:   Batch size = 64
2023-05-06 20:38:32,070:INFO:   Num steps = 11
2023-05-06 20:38:32,074:WARNING: Eval under the multi-sentence per video clip setting.
2023-05-06 20:38:32,074:WARNING: sentence num: 670, video num: 670
2023-05-06 20:38:58,836:INFO: before reshape, sim matrix size: 670 x 670
2023-05-06 20:38:58,843:INFO: after reshape, sim matrix size: 670 x 1 x 670
2023-05-06 20:38:59,116:INFO: Text-to-Video:
2023-05-06 20:38:59,117:INFO: 	>>>  R@1: 50.6 - R@5: 81.2 - R@10: 89.3 - Median R: 1.0 - Mean R: 4.9
2023-05-06 20:38:59,117:INFO: Video-to-Text:
2023-05-06 20:38:59,121:INFO: 	>>>  V2T$R@1: 51.3 - V2T$R@5: 83.0 - V2T$R@10: 89.4 - V2T$Median R: 1.0 - V2T$Mean R: 5.2
2023-05-06 20:40:18,341:INFO: device: cuda:0 n_gpu: 1
2023-05-06 20:40:18,646:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-06 20:40:19,075:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-06 20:40:19,076:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-06 20:40:19,077:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-06 20:40:19,077:WARNING: 	 embed_dim: 512
2023-05-06 20:40:19,078:WARNING: 	 image_resolution: 224
2023-05-06 20:40:19,078:WARNING: 	 vision_layers: 12
2023-05-06 20:40:19,078:WARNING: 	 vision_width: 768
2023-05-06 20:40:19,078:WARNING: 	 vision_patch_size: 32
2023-05-06 20:40:19,078:WARNING: 	 context_length: 77
2023-05-06 20:40:19,078:WARNING: 	 vocab_size: 49408
2023-05-06 20:40:19,078:WARNING: 	 transformer_width: 512
2023-05-06 20:40:19,078:WARNING: 	 transformer_heads: 8
2023-05-06 20:40:19,078:WARNING: 	 transformer_layers: 12
2023-05-06 20:40:19,078:WARNING: 	 cut_top_layer: 0
2023-05-06 20:40:22,146:WARNING: 	 sim_type: seqTransf
2023-05-06 20:40:30,914:INFO: --------------------
2023-05-06 20:40:34,063:INFO: ***** Running test *****
2023-05-06 20:40:34,064:INFO:   Num examples = 670
2023-05-06 20:40:34,064:INFO:   Batch size = 64
2023-05-06 20:40:34,064:INFO:   Num steps = 11
2023-05-06 20:40:34,067:WARNING: Eval under the multi-sentence per video clip setting.
2023-05-06 20:40:34,067:WARNING: sentence num: 670, video num: 670
2023-05-06 20:41:00,243:INFO: before reshape, sim matrix size: 670 x 670
2023-05-06 20:41:00,250:INFO: after reshape, sim matrix size: 670 x 1 x 670
2023-05-06 20:41:00,521:INFO: Text-to-Video:
2023-05-06 20:41:00,521:INFO: 	>>>  R@1: 50.1 - R@5: 79.9 - R@10: 89.7 - Median R: 1.0 - Mean R: 5.7
2023-05-06 20:41:00,521:INFO: Video-to-Text:
2023-05-06 20:41:00,521:INFO: 	>>>  V2T$R@1: 50.9 - V2T$R@5: 81.2 - V2T$R@10: 88.1 - V2T$Median R: 1.0 - V2T$Mean R: 5.6
2023-05-06 20:54:01,865:INFO: device: cuda:0 n_gpu: 1
2023-05-06 20:54:02,171:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-06 20:54:02,576:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-06 20:54:02,577:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-06 20:54:02,577:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-06 20:54:02,577:WARNING: 	 embed_dim: 512
2023-05-06 20:54:02,577:WARNING: 	 image_resolution: 224
2023-05-06 20:54:02,577:WARNING: 	 vision_layers: 12
2023-05-06 20:54:02,577:WARNING: 	 vision_width: 768
2023-05-06 20:54:02,577:WARNING: 	 vision_patch_size: 32
2023-05-06 20:54:02,577:WARNING: 	 context_length: 77
2023-05-06 20:54:02,577:WARNING: 	 vocab_size: 49408
2023-05-06 20:54:02,577:WARNING: 	 transformer_width: 512
2023-05-06 20:54:02,578:WARNING: 	 transformer_heads: 8
2023-05-06 20:54:02,578:WARNING: 	 transformer_layers: 12
2023-05-06 20:54:02,578:WARNING: 	 cut_top_layer: 0
2023-05-06 20:54:05,914:WARNING: 	 sim_type: seqTransf
2023-05-06 20:54:14,890:INFO: --------------------
2023-05-06 20:54:18,056:INFO: ***** Running test *****
2023-05-06 20:54:18,057:INFO:   Num examples = 670
2023-05-06 20:54:18,057:INFO:   Batch size = 64
2023-05-06 20:54:18,057:INFO:   Num steps = 11
2023-05-06 20:54:18,060:WARNING: Eval under the multi-sentence per video clip setting.
2023-05-06 20:54:18,060:WARNING: sentence num: 670, video num: 670
2023-05-06 20:54:44,601:INFO: before reshape, sim matrix size: 670 x 670
2023-05-06 20:54:44,608:INFO: after reshape, sim matrix size: 670 x 1 x 670
2023-05-06 20:54:44,851:INFO: Text-to-Video:
2023-05-06 20:54:44,852:INFO: 	>>>  R@1: 50.6 - R@5: 80.3 - R@10: 89.6 - Median R: 1.0 - Mean R: 4.7
2023-05-06 20:54:44,852:INFO: Video-to-Text:
2023-05-06 20:54:44,852:INFO: 	>>>  V2T$R@1: 49.3 - V2T$R@5: 82.1 - V2T$R@10: 87.9 - V2T$Median R: 2.0 - V2T$Mean R: 5.6
2023-05-06 21:02:53,010:INFO: device: cuda:0 n_gpu: 1
2023-05-06 21:02:53,328:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-06 21:02:53,802:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-06 21:02:53,803:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-06 21:02:53,803:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-06 21:02:53,803:WARNING: 	 embed_dim: 512
2023-05-06 21:02:53,803:WARNING: 	 image_resolution: 224
2023-05-06 21:02:53,803:WARNING: 	 vision_layers: 12
2023-05-06 21:02:53,803:WARNING: 	 vision_width: 768
2023-05-06 21:02:53,804:WARNING: 	 vision_patch_size: 32
2023-05-06 21:02:53,804:WARNING: 	 context_length: 77
2023-05-06 21:02:53,804:WARNING: 	 vocab_size: 49408
2023-05-06 21:02:53,804:WARNING: 	 transformer_width: 512
2023-05-06 21:02:53,804:WARNING: 	 transformer_heads: 8
2023-05-06 21:02:53,804:WARNING: 	 transformer_layers: 12
2023-05-06 21:02:53,804:WARNING: 	 cut_top_layer: 0
2023-05-06 21:02:57,285:WARNING: 	 sim_type: seqTransf
2023-05-06 21:03:06,466:INFO: --------------------
2023-05-06 21:03:09,839:INFO: ***** Running test *****
2023-05-06 21:03:09,839:INFO:   Num examples = 670
2023-05-06 21:03:09,839:INFO:   Batch size = 64
2023-05-06 21:03:09,839:INFO:   Num steps = 11
2023-05-06 21:03:09,842:WARNING: Eval under the multi-sentence per video clip setting.
2023-05-06 21:03:09,842:WARNING: sentence num: 670, video num: 670
2023-05-06 21:03:36,886:INFO: before reshape, sim matrix size: 670 x 670
2023-05-06 21:03:36,894:INFO: after reshape, sim matrix size: 670 x 1 x 670
2023-05-06 21:03:37,120:INFO: Text-to-Video:
2023-05-06 21:03:37,120:INFO: 	>>>  R@1: 41.6 - R@5: 71.3 - R@10: 81.5 - Median R: 2.0 - Mean R: 15.0
2023-05-06 21:03:37,120:INFO: Video-to-Text:
2023-05-06 21:03:37,120:INFO: 	>>>  V2T$R@1: 40.5 - V2T$R@5: 71.4 - V2T$R@10: 81.1 - V2T$Median R: 2.0 - V2T$Mean R: 10.7
2023-05-06 21:09:29,888:INFO: device: cuda:0 n_gpu: 1
2023-05-06 21:09:30,188:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-06 21:09:30,591:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-06 21:09:30,592:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-06 21:09:30,592:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-06 21:09:30,592:WARNING: 	 embed_dim: 512
2023-05-06 21:09:30,592:WARNING: 	 image_resolution: 224
2023-05-06 21:09:30,592:WARNING: 	 vision_layers: 12
2023-05-06 21:09:30,592:WARNING: 	 vision_width: 768
2023-05-06 21:09:30,592:WARNING: 	 vision_patch_size: 32
2023-05-06 21:09:30,592:WARNING: 	 context_length: 77
2023-05-06 21:09:30,592:WARNING: 	 vocab_size: 49408
2023-05-06 21:09:30,592:WARNING: 	 transformer_width: 512
2023-05-06 21:09:30,592:WARNING: 	 transformer_heads: 8
2023-05-06 21:09:30,592:WARNING: 	 transformer_layers: 12
2023-05-06 21:09:30,592:WARNING: 	 cut_top_layer: 0
2023-05-06 21:09:33,664:WARNING: 	 sim_type: seqTransf
2023-05-06 21:09:42,378:INFO: --------------------
2023-05-06 21:12:21,290:INFO: device: cuda:0 n_gpu: 1
2023-05-06 21:12:21,604:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-06 21:12:22,020:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-06 21:12:22,020:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-06 21:12:22,021:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-06 21:12:22,021:WARNING: 	 embed_dim: 512
2023-05-06 21:12:22,021:WARNING: 	 image_resolution: 224
2023-05-06 21:12:22,021:WARNING: 	 vision_layers: 12
2023-05-06 21:12:22,021:WARNING: 	 vision_width: 768
2023-05-06 21:12:22,021:WARNING: 	 vision_patch_size: 32
2023-05-06 21:12:22,021:WARNING: 	 context_length: 77
2023-05-06 21:12:22,021:WARNING: 	 vocab_size: 49408
2023-05-06 21:12:22,021:WARNING: 	 transformer_width: 512
2023-05-06 21:12:22,021:WARNING: 	 transformer_heads: 8
2023-05-06 21:12:22,021:WARNING: 	 transformer_layers: 12
2023-05-06 21:12:22,021:WARNING: 	 cut_top_layer: 0
2023-05-06 21:12:24,876:WARNING: 	 sim_type: seqTransf
2023-05-06 21:12:33,461:INFO: --------------------
2023-05-06 21:12:36,646:INFO: ***** Running test *****
2023-05-06 21:12:36,646:INFO:   Num examples = 670
2023-05-06 21:12:36,646:INFO:   Batch size = 64
2023-05-06 21:12:36,646:INFO:   Num steps = 11
2023-05-06 21:12:36,649:WARNING: Eval under the multi-sentence per video clip setting.
2023-05-06 21:12:36,649:WARNING: sentence num: 670, video num: 670
2023-05-06 21:13:02,716:INFO: before reshape, sim matrix size: 670 x 670
2023-05-06 21:13:02,722:INFO: after reshape, sim matrix size: 670 x 1 x 670
2023-05-06 21:13:02,966:INFO: Text-to-Video:
2023-05-06 21:13:02,966:INFO: 	>>>  R@1: 11.8 - R@5: 28.2 - R@10: 38.1 - Median R: 24.0 - Mean R: 92.2
2023-05-06 21:13:02,966:INFO: Video-to-Text:
2023-05-06 21:13:02,966:INFO: 	>>>  V2T$R@1: 12.5 - V2T$R@5: 25.7 - V2T$R@10: 34.9 - V2T$Median R: 27.5 - V2T$Mean R: 73.3
2023-05-06 21:19:37,625:INFO: device: cuda:0 n_gpu: 1
2023-05-06 21:19:37,925:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-06 21:19:38,343:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-06 21:19:38,343:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-06 21:19:38,343:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-06 21:19:38,344:WARNING: 	 embed_dim: 512
2023-05-06 21:19:38,344:WARNING: 	 image_resolution: 224
2023-05-06 21:19:38,344:WARNING: 	 vision_layers: 12
2023-05-06 21:19:38,344:WARNING: 	 vision_width: 768
2023-05-06 21:19:38,344:WARNING: 	 vision_patch_size: 32
2023-05-06 21:19:38,344:WARNING: 	 context_length: 77
2023-05-06 21:19:38,344:WARNING: 	 vocab_size: 49408
2023-05-06 21:19:38,344:WARNING: 	 transformer_width: 512
2023-05-06 21:19:38,344:WARNING: 	 transformer_heads: 8
2023-05-06 21:19:38,344:WARNING: 	 transformer_layers: 12
2023-05-06 21:19:38,344:WARNING: 	 cut_top_layer: 0
2023-05-06 21:19:41,432:WARNING: 	 sim_type: seqTransf
2023-05-06 21:19:50,173:INFO: --------------------
2023-05-06 21:19:53,334:INFO: ***** Running test *****
2023-05-06 21:19:53,334:INFO:   Num examples = 670
2023-05-06 21:19:53,334:INFO:   Batch size = 64
2023-05-06 21:19:53,334:INFO:   Num steps = 11
2023-05-06 21:19:53,338:WARNING: Eval under the multi-sentence per video clip setting.
2023-05-06 21:19:53,338:WARNING: sentence num: 670, video num: 670
2023-05-06 21:20:19,688:INFO: before reshape, sim matrix size: 670 x 670
2023-05-06 21:20:19,697:INFO: after reshape, sim matrix size: 670 x 1 x 670
2023-05-06 21:20:19,936:INFO: Text-to-Video:
2023-05-06 21:20:19,936:INFO: 	>>>  R@1: 36.9 - R@5: 66.0 - R@10: 76.4 - Median R: 3.0 - Mean R: 15.1
2023-05-06 21:20:19,936:INFO: Video-to-Text:
2023-05-06 21:20:19,937:INFO: 	>>>  V2T$R@1: 36.8 - V2T$R@5: 64.5 - V2T$R@10: 74.9 - V2T$Median R: 3.0 - V2T$Mean R: 11.9
2023-05-18 19:52:35,099:INFO: device: cuda:0 n_gpu: 2
2023-05-18 19:52:35,413:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-18 19:52:35,834:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-18 19:52:35,835:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-18 19:52:35,835:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-18 19:52:35,835:WARNING: 	 embed_dim: 512
2023-05-18 19:52:35,835:WARNING: 	 image_resolution: 224
2023-05-18 19:52:35,836:WARNING: 	 vision_layers: 12
2023-05-18 19:52:35,836:WARNING: 	 vision_width: 768
2023-05-18 19:52:35,836:WARNING: 	 vision_patch_size: 32
2023-05-18 19:52:35,836:WARNING: 	 context_length: 77
2023-05-18 19:52:35,836:WARNING: 	 vocab_size: 49408
2023-05-18 19:52:35,836:WARNING: 	 transformer_width: 512
2023-05-18 19:52:35,836:WARNING: 	 transformer_heads: 8
2023-05-18 19:52:35,836:WARNING: 	 transformer_layers: 12
2023-05-18 19:52:35,836:WARNING: 	 cut_top_layer: 0
2023-05-18 19:52:38,137:WARNING: 	 sim_type: seqTransf
2023-05-18 19:52:46,068:INFO: --------------------
2023-05-18 19:56:12,273:INFO: device: cuda:0 n_gpu: 2
2023-05-18 19:56:12,582:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-18 19:56:13,015:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-18 19:56:13,015:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-18 19:56:13,015:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-18 19:56:13,016:WARNING: 	 embed_dim: 512
2023-05-18 19:56:13,016:WARNING: 	 image_resolution: 224
2023-05-18 19:56:13,016:WARNING: 	 vision_layers: 12
2023-05-18 19:56:13,016:WARNING: 	 vision_width: 768
2023-05-18 19:56:13,016:WARNING: 	 vision_patch_size: 32
2023-05-18 19:56:13,016:WARNING: 	 context_length: 77
2023-05-18 19:56:13,016:WARNING: 	 vocab_size: 49408
2023-05-18 19:56:13,016:WARNING: 	 transformer_width: 512
2023-05-18 19:56:13,016:WARNING: 	 transformer_heads: 8
2023-05-18 19:56:13,016:WARNING: 	 transformer_layers: 12
2023-05-18 19:56:13,016:WARNING: 	 cut_top_layer: 0
2023-05-18 19:56:15,728:WARNING: 	 sim_type: seqTransf
2023-05-18 19:56:23,824:INFO: --------------------
2023-05-18 19:56:26,884:INFO: ***** Running test *****
2023-05-18 19:56:26,885:INFO:   Num examples = 670
2023-05-18 19:56:26,885:INFO:   Batch size = 64
2023-05-18 19:56:26,885:INFO:   Num steps = 11
2023-05-18 19:56:26,888:WARNING: Eval under the multi-sentence per video clip setting.
2023-05-18 19:56:26,888:WARNING: sentence num: 670, video num: 670
2023-05-18 19:58:17,732:INFO: device: cuda:0 n_gpu: 2
2023-05-18 19:58:18,051:INFO: Model loaded from ../weights/clip2video/msvd/pytorch_model.bin.2
2023-05-18 19:58:18,480:INFO: loading archive file /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base
2023-05-18 19:58:18,480:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 77,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2023-05-18 19:58:18,481:INFO: Weight doesn't exsits. /playpen-storage/avinashm/Experiments/compositionality/models/CLIP2Video/modules/cross-base/cross_pytorch_model.bin
2023-05-18 19:58:18,481:WARNING: 	 embed_dim: 512
2023-05-18 19:58:18,481:WARNING: 	 image_resolution: 224
2023-05-18 19:58:18,481:WARNING: 	 vision_layers: 12
2023-05-18 19:58:18,481:WARNING: 	 vision_width: 768
2023-05-18 19:58:18,481:WARNING: 	 vision_patch_size: 32
2023-05-18 19:58:18,481:WARNING: 	 context_length: 77
2023-05-18 19:58:18,481:WARNING: 	 vocab_size: 49408
2023-05-18 19:58:18,481:WARNING: 	 transformer_width: 512
2023-05-18 19:58:18,481:WARNING: 	 transformer_heads: 8
2023-05-18 19:58:18,481:WARNING: 	 transformer_layers: 12
2023-05-18 19:58:18,481:WARNING: 	 cut_top_layer: 0
2023-05-18 19:58:21,013:WARNING: 	 sim_type: seqTransf
2023-05-18 19:58:29,063:INFO: --------------------
2023-05-18 19:58:32,174:INFO: ***** Running test *****
2023-05-18 19:58:32,174:INFO:   Num examples = 670
2023-05-18 19:58:32,174:INFO:   Batch size = 64
2023-05-18 19:58:32,174:INFO:   Num steps = 11
2023-05-18 19:58:32,178:WARNING: Eval under the multi-sentence per video clip setting.
2023-05-18 19:58:32,178:WARNING: sentence num: 670, video num: 670
2023-05-18 19:59:00,642:INFO: before reshape, sim matrix size: 670 x 670
2023-05-18 19:59:00,652:INFO: after reshape, sim matrix size: 670 x 1 x 670
2023-05-18 19:59:00,810:INFO: Text-to-Video:
2023-05-18 19:59:00,810:INFO: 	>>>  R@1: 50.6 - R@5: 83.4 - R@10: 91.3 - Median R: 1.0 - Mean R: 4.6
2023-05-18 19:59:00,810:INFO: Video-to-Text:
2023-05-18 19:59:00,810:INFO: 	>>>  V2T$R@1: 51.6 - V2T$R@5: 84.4 - V2T$R@10: 91.7 - V2T$Median R: 1.0 - V2T$Mean R: 4.8
