{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5366622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805061b-fe9f-447a-9bda-209f669f5731",
   "metadata": {},
   "source": [
    "## Load Perspective API scores and calculate MaxToxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f12d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_perspective_scores(output_dir, dataset_type):\n",
    "    files = [i for i in os.listdir(output_dir) if i.startswith('perspective_api_all_occupations_all_gens') and i.endswith('.csv')]\n",
    "    perspective_scores = pd.concat([pd.read_csv(os.path.join(output_dir,i)) for i in files])\n",
    "    if 'dataset_type' not in perspective_scores.columns:\n",
    "        perspective_scores = perspective_scores.assign(dataset_type = dataset_type)\n",
    "    \n",
    "    perspective_scores['model_name'] = perspective_scores['model_name'].apply(lambda x: x.split('/')[-1].replace('-hf',''))\n",
    "    perspective_scores['prompt_original'] = perspective_scores['prompt']\n",
    "    perspective_scores['prompt'] = perspective_scores['prompt'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "    perspective_scores = perspective_scores.assign(seed = perspective_scores['prompt_original'].apply(lambda x: x.split('_')[-1]))\n",
    "    \n",
    "    if 'dataset_type' in perspective_scores.columns:\n",
    "        print(perspective_scores[['model_name','dataset_type']].value_counts())\n",
    "    else:\n",
    "        print(perspective_scores['model_name'].value_counts())\n",
    "\n",
    "    return perspective_scores\n",
    "\n",
    "\n",
    "def format_df_agg(df_agg, agg_cols, col, dataset, prompt):\n",
    "    df_agg = df_agg.describe(percentiles=[i / 100 for i in range(5,100,5)]).transpose().iloc[1:,]\n",
    "    df_agg = df_agg[agg_cols]\n",
    "    df_agg.index = df_agg.index.rename('model')\n",
    "    df_agg = df_agg.reset_index()\n",
    "    df_agg = df_agg.assign(score = col, dataset = dataset, prompt = prompt)[['score','dataset', 'prompt', 'model'] + agg_cols]\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def calc_max_toxicity(perspective_scores):\n",
    "    prompts = perspective_scores['prompt'].unique()\n",
    "    datasets = perspective_scores['dataset_type'].unique()\n",
    "    score_cols = ['TOXICITY']\n",
    "    # score_cols = ['TOXICITY', 'INSULT','IDENTITY_ATTACK','FLIRTATION']\n",
    "    agg_cols = ['mean', 'std', '25%', '50%', '75%', '90%', 'max']\n",
    "    \n",
    "    df_list = []\n",
    "    df_diff_list = []\n",
    "    df_max_id_list = []\n",
    "    for col in tqdm(score_cols):\n",
    "        for dataset in datasets:\n",
    "            for prompt in prompts:\n",
    "                df_agg = perspective_scores[(perspective_scores['prompt'] == prompt) & (perspective_scores['dataset_type'] == dataset)]\n",
    "                df_agg = pd.pivot_table(df_agg, values=col, columns='model_name', index=['filename','rank','im_seed','im_index','prompt_original']).reset_index()\n",
    "                df_agg_max = df_agg.groupby(['filename','rank','im_seed','prompt_original']).agg('max')\n",
    "                df_agg_min = df_agg.groupby(['filename','rank','im_seed','prompt_original']).agg('min')\n",
    "                df_agg_diff = df_agg_max - df_agg_min\n",
    "\n",
    "                df_agg_id_max = df_agg.groupby(['filename','rank','im_seed','prompt_original']).idxmax()\n",
    "                del df_agg_id_max['im_index']\n",
    "                for i in df_agg_id_max.columns:\n",
    "                    df_agg_id_max.loc[:,i] = df_agg_id_max[i].apply(lambda x: df_agg.iloc[int(x)]['im_index'])\n",
    "                df_agg_id_max = df_agg_id_max.reset_index()\n",
    "                df_agg_id_max = df_agg_id_max.assign(score = col, dataset = dataset, prompt = prompt)\n",
    "\n",
    "                df_agg = format_df_agg(df_agg_max, agg_cols, col, dataset, prompt)\n",
    "                df_agg_diff = format_df_agg(df_agg_diff, agg_cols, col, dataset, prompt)\n",
    "                \n",
    "                df_list.append(df_agg)\n",
    "                df_diff_list.append(df_agg_diff)\n",
    "                df_max_id_list.append(df_agg_id_max)\n",
    "\n",
    "    return [pd.concat(df_list), pd.concat(df_diff_list), pd.concat(df_max_id_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36cc9659-b002-4dfe-85a0-0f509a704987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name    dataset_type\n",
      "llava-1.5-7b  race_gender     11526\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "perspective_scores = load_perspective_scores('output/', 'race_gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d114874-3d9d-459e-8ab3-24a404edd6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.41it/s]\n"
     ]
    }
   ],
   "source": [
    "max_toxicity, max_toxicity_diff, max_toxicity_id = calc_max_toxicity(perspective_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd01f83c-766a-4bb5-9f2b-3de706a097e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>prompt</th>\n",
       "      <th>model</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>90%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TOXICITY</td>\n",
       "      <td>race_gender</td>\n",
       "      <td>keywords_characteristics</td>\n",
       "      <td>llava-1.5-7b</td>\n",
       "      <td>0.139273</td>\n",
       "      <td>0.071592</td>\n",
       "      <td>0.091913</td>\n",
       "      <td>0.113987</td>\n",
       "      <td>0.178917</td>\n",
       "      <td>0.254629</td>\n",
       "      <td>0.426917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score      dataset                    prompt         model      mean  \\\n",
       "0  TOXICITY  race_gender  keywords_characteristics  llava-1.5-7b  0.139273   \n",
       "\n",
       "        std       25%       50%       75%       90%       max  \n",
       "0  0.071592  0.091913  0.113987  0.178917  0.254629  0.426917  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_toxicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b97f00-a0e6-468a-9dda-3a87eeae9e39",
   "metadata": {},
   "source": [
    "## Count frequency of competency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff245a5-78b7-4441-a6f4-340951d96ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = pd.read_csv('output/generations.csv')\n",
    "generations = generations.assign(text_tokens = generations['keywords_characteristics_0'].str.lower().str.replace(',','').str.replace('.','').str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e97c8c-b5c0-433f-8634-4a3c63fda3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "competence_tokens = []\n",
    "with open('competency_words.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        competence_tokens.append(line.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df783ad-c996-4e0d-9764-7d0ea56d4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = generations.assign(competence_words = generations.apply(lambda x: set(x['text_tokens']).intersection(competence_tokens), axis=1))\n",
    "generations = generations.assign(competence_count = generations['competence_words'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79a8b1a8-ea8e-4126-b8b3-a5c2ee836faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11526.000000\n",
       "mean         0.183498\n",
       "std          0.432207\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          4.000000\n",
       "Name: competence_count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations['competence_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03b48a-4264-46ae-bf57-41f61a777759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
